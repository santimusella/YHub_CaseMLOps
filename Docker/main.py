# -*- coding: utf-8 -*-
"""YHub_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZK7hoTgQ8urxZeQvPeah-yqTZo9YStC4

### Imports
"""

# Necessary imports
# Imports necessarios
from sklearn import datasets
import pandas as pd
import numpy as np
import seaborn as sns
import pyspark.sql
from pyspark.ml.linalg import Vectors
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.regression import LinearRegression

"""### Data Loading - Verification - Exploration"""

# Load the data from sklearn dataset
# Carregar a data inicial do sklearn dataset
boston_initial_data = datasets.load_boston()

# Print the description of the data
# Print a descricao da data
print(boston_initial_data.DESCR)

# Print the feature names (possible columns in the future)
# Print as feature names (possivelmente serao colunas depois)
print(boston_initial_data.feature_names)
# We can see from the print statement that the 'MEDV' col is missing
# Podemos ver que a coluna 'MEDV' nao esta nos feature names

# Print all keys of the dataset for further exploration
# Print todas as keys do dataset para explorar a data
print(boston_initial_data.keys())

# !!! The 'target' key corresponds to the missing MEDV col !!!
# !!! A key 'target' corresponde a coluna MEDV !!!

# Create a pandas df with the dataset
# Criar um pandas df com o dataset original
boston_df = pd.DataFrame(boston_initial_data.data, columns=boston_initial_data.feature_names)

# Add the target col as MEDV
# Adicionar a coluna target como MEDV
boston_df['MEDV'] = boston_initial_data.target

# Check the first 5 rows from the df
# Verificar as primeiras filas do df
boston_df.head()

# !!! DATA  OK !!!

# Second check - null values
# Segunda verificacao - valores nulos
boston_df.isnull().sum()

# !!! DATA OK !!!

"""Now that we've checked that the data OK, we can explore it.

Agora que verificamos que a data esta OK, podemos comecar a explorar o dataset.
"""

# Using seaborn histplot (kde=true gives us the curve)
# Usando seaborn histplot (kde=true inclui a curva no grafico)
sns.histplot(boston_df['MEDV'], kde=True)

# We can see that there is a normal distribution with some outliers, meaning 
# that we should use MSE as a loss function for the linear regression
# Podemos ver que existe uma distribuicao normal com alguns valores atipicos,
# entao  deveriamos usar EQM (MSE) como a funcao para a regressao linear

"""Since we have so many possible variables for our linear regression model, we should create a correlation matrix to check which variable has the highest positive correlation and lowest negative correlation with out target (MEDV). We will create three models, two will use the highest and lowest correlation variables, and the other model will use the VectorAssembler transformer from PySpark to combine all variables into one, and use this single merged variable.

Como temos tantas variaveis possiveis para o modelo de regressao linear, 
podemos criar uma matriz de correlacao para verificar as variaveis e encontrar
a variavel com a maior correlacao positiva e menor correlacao negativa em relacao ao target (MEDV). Vamos criar tres modelos diferentes; dois serao com as variaveis com maior e menor correlacao, e o terceiro modelo usara a funcao VectorAssembler do PySpark para combinar todas as variaveis em uma so, e usar essa variavel combinada no modelo.
"""

# Create a correlation matrix
# Matriz de correlacao
correlation = boston_df.corr()

# Print the correlations between all vars and MEDV (sorted)
# Print as correlacoes entre as variaveis e MEDV (em ordem)
print(correlation['MEDV'].sort_values(ascending=False))

# !!! HIGHEST - RM /// LOWEST - LSTAT !!!
# !!! MAIOR - RM /// MENOR - LSTAT !!!

"""### Functions / Funcoes"""

def splitDatasets(df, train, test):
  training_data, test_data = df.randomSplit([train,test])
  return training_data, test_data

def vectorAssemble(df, in_cols, out, target):
  # Using the VectorAssembler function to merge all columns together
  # Usando a funcao VectorAssembler para combinar todas as variaveis
  assembler = VectorAssembler(inputCols=in_cols, outputCol=out)

  # Combining the new Features col to our df, then removing unecessary cols
  # Combinando a nova coluna Features com a nossa df, e removendo as outras cols
  vector_df = assembler.transform(df).select([out,target])
  return vector_df

def createLinearRegModel(df, features, label):
  # Now we use the LinearRegression function (using MSE as loss function)
  # Agora usamos LinearRegression, usando MSE como a loss function
  model = LinearRegression(featuresCol = features, labelCol = label)

  # Fit the training data into the lin regression model
  # Fazemos o ajuste da data no modelo de regressao linear
  model = model.fit(df)

  return model

"""### SPARKSESSION - DATAFRAME"""

# Now we will start using PySpark. We need to create a SparkSession
# Agora vamos comecar a usar pyspark. Precisamos criar uma SparkSession
spark = pyspark.sql.SparkSession \
    .builder.master('local[16]') \
    .appName("Linear Regression Model YHub") \
    .getOrCreate()

# SparkSession syntax: 
# master(local[x]) --> This is the master node, running locally with 16 cores (partitions)
# appname () --> Name of the sparksession app

# Create a spark df for the boston df
# Criar uma spark df com a data do boston df
boston_spark_df = spark.createDataFrame(boston_df)

"""### VectorAssemble Model"""

# Using VectorAssemble to combine all variables
# Usando VectorAssemble para combinar todas as variaveis
vector_boston_spark_df = vectorAssemble(boston_spark_df, \
                                        ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT'], \
                                        'FEATURES', 'MEDV')

# Split df into two; training and test data (75/25)
# Separar a df em dois; data de treino e teste (75/25)
vector_training_data, vector_test_data = splitDatasets(vector_boston_spark_df,0.75,0.25)

vector_model = createLinearRegModel(vector_training_data, 'FEATURES', 'MEDV')

# Create a LinearRegressionSummary object using the evaluate method
# Criar um objeto LinearRegressionSummary usando o metodo "evaluate"
vector_test_predictions = vector_model.evaluate(vector_test_data)
vector_test_predictions.predictions.show(10)

# Check the r-squared value of the model
# Verificando o valor do r ao quadrado do modelo
vector_training_r2 = vector_model.summary.r2
print("Vector training r-squared: {}".format(vector_training_r2))

# Check the RMSE value of the model
# Verificando o valor do erro quadratico medio do modelo
vector_training_rmse = vector_model.summary.rootMeanSquaredError
print("Vector training RMSE:      {}".format(vector_training_rmse))

# Check the r-squared value of the model
# Verificando o valor do r ao quadrado do modelo
vector_test_r2 = vector_test_predictions.r2
print("Vector test r-squared:     {}".format(vector_test_r2))

# Check the r-squared value of the model
# Verificando o valor do r ao quadrado do modelo
vector_test_rmse = vector_test_predictions.rootMeanSquaredError
print("Vector test RMSE:          {}\n".format(vector_test_rmse))

vector_r2_difference = vector_test_r2 - vector_training_r2
vector_rmse_difference = vector_test_rmse - vector_training_rmse
print("Vector model r-squared difference: {}".format(vector_r2_difference))
print("Vector model RMSE difference:      {}".format(vector_rmse_difference))

"""### RM Model"""

# Using the RM variable 
rm_boston_spark_df = boston_spark_df.select(['RM','MEDV'])

rm_boston_spark_df = vectorAssemble(boston_spark_df, ['RM'], 'vRM', 'MEDV')

# Split df into two; training and test data (75/25)
# Separar a df em dois; data de treino e teste (75/25)
rm_training_data, rm_test_data = splitDatasets(rm_boston_spark_df,0.75,0.25)

rm_model = createLinearRegModel(rm_training_data, 'vRM', 'MEDV')

# Create a LinearRegressionSummary object using the evaluate method
# Criar um objeto LinearRegressionSummary usando o metodo "evaluate"
rm_test_predictions = rm_model.evaluate(rm_test_data)
rm_test_predictions.predictions.show(10)

# Check the r-squared value of the model
# Verificando o valor do r ao quadrado do modelo
rm_training_r2 = rm_model.summary.r2
print("RM training r-squared: {}".format(rm_training_r2))

# Check the RMSE value of the model
# Verificando o valor do erro quadratico medio do modelo
rm_training_rmse = rm_model.summary.rootMeanSquaredError
print("RM training RMSE:      {}".format(rm_training_rmse))

# Check the r-squared value of the model
# Verificando o valor do r ao quadrado do modelo
rm_test_r2 = rm_test_predictions.r2
print("RM test r-squared:     {}".format(rm_test_r2))

# Check the r-squared value of the model
# Verificando o valor do r ao quadrado do modelo
rm_test_rmse = rm_test_predictions.rootMeanSquaredError
print("RM test RMSE:          {}\n".format(rm_test_rmse))

rm_r2_difference = rm_test_r2 - rm_training_r2
rm_rmse_difference = rm_test_rmse - rm_training_rmse
print("RM model r-squared difference: {}".format(rm_r2_difference))
print("RM model RMSE difference:      {}".format(rm_rmse_difference))

"""### LSTAT MODEL"""

# Using the LSTAT variable
lstat_boston_spark_df = boston_spark_df.select(['LSTAT','MEDV'])

lstat_boston_spark_df = vectorAssemble(boston_spark_df, ['LSTAT'], 'vLSTAT', 'MEDV')

# Split df into two; training and test data (75/25)
# Separar a df em dois; data de treino e teste (75/25)
lstat_training_data, lstat_test_data = splitDatasets(lstat_boston_spark_df,0.75,0.25)

lstat_model = createLinearRegModel(lstat_training_data, 'vLSTAT', 'MEDV')

# Create a LinearRegressionSummary object using the evaluate method
# Criar um objeto LinearRegressionSummary usando o metodo "evaluate"
lstat_test_predictions = lstat_model.evaluate(lstat_test_data)
lstat_test_predictions.predictions.show(10)

# Check the r-squared value of the model
# Verificando o valor do r ao quadrado do modelo
lstat_training_r2 = lstat_model.summary.r2
print("LSTAT training r-squared: {}".format(lstat_training_r2))

# Check the RMSE value of the model
# Verificando o valor do erro quadratico medio do modelo
lstat_training_rmse = lstat_model.summary.rootMeanSquaredError
print("LSTAT training RMSE:      {}".format(lstat_training_rmse))

# Check the r-squared value of the model
# Verificando o valor do r ao quadrado do modelo
lstat_test_r2 = lstat_test_predictions.r2
print("LSTAT test r-squared:     {}".format(lstat_test_r2))

# Check the r-squared value of the model
# Verificando o valor do r ao quadrado do modelo
lstat_test_rmse = lstat_test_predictions.rootMeanSquaredError
print("LSTAT test RMSE:          {}\n".format(lstat_test_rmse))

lstat_r2_difference = lstat_test_r2 - lstat_training_r2
lstat_rmse_difference = lstat_test_rmse - lstat_training_rmse
print("LSTAT model r-squared difference: {}".format(lstat_r2_difference))
print("LSTAT model RMSE difference:      {}".format(lstat_rmse_difference))

"""### GRAPHS / GRAFICOS"""

vector_pandas_df = vector_test_predictions.predictions.toPandas()
sns.regplot(x="prediction", y="MEDV", data=vector_pandas_df)

rm_pandas_df = rm_test_predictions.predictions.toPandas()
sns.regplot(x="prediction", y="MEDV", data=rm_pandas_df)

lstat_pandas_df = lstat_test_predictions.predictions.select(['prediction','MEDV']).toPandas()
sns.regplot(x="prediction", y="MEDV", data=lstat_pandas_df)

"""## Results
We had three different models; a vectorassemble model which combines all variables into one, a model which compares the RM variable (average rooms per dwelling) with the target, and finally a model using the LSTAT variable (% lower status of the population). 

From our results after running the training and test data in our models, we have that the best fit is the model using VectorAssemble, combining all variables into a single feature. It has the highest r-squared value and lowest RMSE value, and from the visual graph we can see the vector model has the best fit.


## Resultados
Comecamos com tres modelos diferentes; um modelo usando VectorAssemble que combinava todas as variaveis em uma so, um modelo que usa a variavel RM que tem a maior correlacao com o alvo, e um ultimo modelo que usa a variavel LSTAT que tem a menor correlacao com o alvo.

Pelos nossos resultados depois de rodar a data de treino e teste em todos os modelos, podemos ver que o modelo com melhor fit seria o modelo que usa VectorAssemble. Ele tem o maior valor de r ao quadrado, e o menor valor do erro quadratico medio. Alem disso, podemos ver com os graficos que esse modelo tem o melhor ajuste.
"""